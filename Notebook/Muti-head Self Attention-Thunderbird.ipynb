{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27ca417",
   "metadata": {},
   "source": [
    "**1. Read csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61abe436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cea5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe5025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c113ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.txt', 'a')\n",
    "f.write('seed: ' + str(seed) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata_all = pd.read_csv(r'~/Python_projects/Rationale/Dataset/Thunderbird.log_structured.csv')\n",
    "logdata = logdata_all[:2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197a6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7910aa0897e6>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: 0 if x == '-' else 1)\n"
     ]
    }
   ],
   "source": [
    "def slide_window(logdata, window_size = 20, step_size = 10):\n",
    "#     logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: int(x != '-'))\n",
    "    logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: 0 if x == '-' else 1)  \n",
    "    data = logdata.loc[:, ['EventId', 'Label']]\n",
    "    data['Key_label'] = data['Label']\n",
    "    data.rename(columns={'Label':'Sequence_label'})\n",
    "    logkey = data['EventId']\n",
    "    logkey_label = data['Key_label']\n",
    "\n",
    "    new_data = []\n",
    "    idx = 0\n",
    "\n",
    "    while idx <= data.shape[0] - window_size:\n",
    "        new_data.append([logkey[idx : idx+window_size].values,\n",
    "                         max(logkey_label[idx : idx+window_size]),\n",
    "                         logkey_label[idx : idx+window_size].values\n",
    "                        ])\n",
    "        idx += step_size\n",
    "    return pd.DataFrame(new_data, columns = ['EventId', 'Sequence_label', 'Key_label'])\n",
    "\n",
    "dataset = slide_window(logdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec473de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194861 5138\n"
     ]
    }
   ],
   "source": [
    "setup_seed()\n",
    "\n",
    "n_labeled = 20\n",
    "n_unlabeled = 2000\n",
    "a_unlabeled = 0\n",
    "\n",
    "normal_data = dataset[dataset['Sequence_label']==0]\n",
    "abnormal_data = dataset[dataset['Sequence_label']==1]\n",
    "print(normal_data.shape[0], abnormal_data.shape[0])\n",
    "\n",
    "# train data\n",
    "train_normal_all = normal_data.sample(n= n_unlabeled + n_labeled, random_state=seed)\n",
    "train_abnormal_all = abnormal_data.sample(n= a_unlabeled + n_labeled, random_state=seed)\n",
    "\n",
    "train_normal_labeled = train_normal_all.sample(n=n_labeled, random_state=seed)\n",
    "train_abnormal_labeled = train_abnormal_all.sample(n=n_labeled, random_state=seed)\n",
    "\n",
    "train_normal_unlabeled = train_normal_all.drop(train_normal_labeled.index)\n",
    "train_abnormal_unlabeled = train_abnormal_all.drop(train_abnormal_labeled.index)\n",
    "train_unlabeled = pd.concat([train_normal_unlabeled, train_abnormal_unlabeled])\n",
    "\n",
    "train_normal_labeled['Semi'] = 0\n",
    "train_abnormal_labeled['Semi'] = 0\n",
    "train_unlabeled['Semi'] = 1\n",
    "train_ds = pd.concat([train_normal_labeled, train_abnormal_labeled, train_unlabeled])\n",
    "\n",
    "rest_normal = normal_data.drop(train_normal_all.index)\n",
    "rest_abnormal = abnormal_data.drop(train_abnormal_all.index)\n",
    "\n",
    "# validation data\n",
    "val_normal = rest_normal.sample(n=200, random_state=seed)\n",
    "val_abnormal = rest_abnormal.sample(n=20, random_state=seed)\n",
    "\n",
    "val_ds = pd.concat([val_normal, val_abnormal])\n",
    "val_ds['Semi'] = 0\n",
    "\n",
    "# testing data\n",
    "test_normal = rest_normal.drop(val_normal.index).sample(n=20000, random_state=seed)\n",
    "test_abnormal = rest_abnormal.drop(val_abnormal.index).sample(n=2000, random_state=seed)\n",
    "\n",
    "test_ds = pd.concat([test_normal, test_abnormal])\n",
    "test_ds['Semi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828dc59",
   "metadata": {},
   "source": [
    "**2. Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1818754",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "for index, row in train_ds.iterrows():\n",
    "    counts.update(row['EventId'])\n",
    "    \n",
    "logkey2index ={\"\":0,\"UNK\":1}\n",
    "logkeys = [\"\",\"UNK\"]\n",
    "\n",
    "for word in counts:\n",
    "    logkey2index[word] = len(logkeys)\n",
    "    logkeys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa010795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, logkey2index):\n",
    "    return np.array([logkey2index.get(logkey, logkey2index[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "train_ds.loc[:,'Encoded'] = train_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))\n",
    "val_ds.loc[:,'Encoded'] = val_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))\n",
    "test_ds.loc[:,'Encoded'] = test_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3cc005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 60\n",
    "batch_size_val = 20\n",
    "batch_size_test = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "train_data_list = []\n",
    "for i in range(train_ds.shape[0]):\n",
    "    train_data_list.append([train_ds.iloc[i, 4].tolist(), train_ds.iloc[i, 1], \\\n",
    "                            train_ds.iloc[i, 2].tolist(), train_ds.iloc[i, 3]])\n",
    "    \n",
    "val_data_list = []\n",
    "for i in range(val_ds.shape[0]):\n",
    "    val_data_list.append([val_ds.iloc[i, 4].tolist(), val_ds.iloc[i, 1], \\\n",
    "                            val_ds.iloc[i, 2].tolist(), val_ds.iloc[i, 3]])\n",
    "    \n",
    "test_data_list = []\n",
    "for i in range(test_ds.shape[0]):\n",
    "    test_data_list.append([test_ds.iloc[i, 4].tolist(), test_ds.iloc[i, 1], \\\n",
    "                            test_ds.iloc[i, 2].tolist(), test_ds.iloc[i, 3]])\n",
    "\n",
    "\n",
    "def collate_fn(data_list):\n",
    "    sequence = torch.tensor([x[0] for x in data_list])\n",
    "    sequence_label = torch.tensor([x[1] for x in data_list])\n",
    "    key_label = torch.tensor([x[2] for x in data_list])\n",
    "    semi = torch.tensor([x[3] for x in data_list])\n",
    "    return sequence, sequence_label, key_label, semi\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size = batch_size_train, collate_fn=collate_fn, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(val_data_list, batch_size = batch_size_val, collate_fn=collate_fn, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size = batch_size_test, collate_fn=collate_fn, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4c905",
   "metadata": {},
   "source": [
    "**3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20998cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "lambda_p = 1\n",
    "hidden_size = 150\n",
    "attention_size = 300\n",
    "n_attention_heads = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba959334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=hidden_size, attention_size=attention_size, n_attention_heads=n_attention_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_size = attention_size\n",
    "        self.n_attention_heads = n_attention_heads\n",
    "        self.W1 = nn.Linear(hidden_size, attention_size, bias=True)\n",
    "        self.W2 = nn.Linear(attention_size, n_attention_heads, bias=True)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        x = torch.tanh(self.W1(hidden))\n",
    "        # x.shape = (batch_size, sentence_length, attention_size)\n",
    "        \n",
    "        x = F.softmax(self.W2(x), dim=1)  # softmax over sentence_length\n",
    "        # x.shape = (batch_size, sentence_length, n_attention_heads)\n",
    "\n",
    "        A = x.transpose(1, 2)\n",
    "        M = A @ hidden\n",
    "        # A.shape = (batch_size, n_attention_heads, sentence_length)\n",
    "        # M.shape = (batch_size, n_attention_heads, hidden_size)\n",
    "\n",
    "        return M, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9d4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVDDNet(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_size, n_attention_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_size = attention_size\n",
    "        self.n_attention_heads = n_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = 1.0\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(logkeys), embedding_dim=hidden_size)      \n",
    "        self.self_attention = SelfAttention(hidden_size=self.hidden_size,\n",
    "                                            attention_size=attention_size,\n",
    "                                            n_attention_heads=n_attention_heads) \n",
    "        self.c_n = nn.Parameter((torch.rand(1, n_attention_heads, self.hidden_size) - 0.5) * 2)\n",
    "        self.c_a = nn.Parameter((torch.rand(1, n_attention_heads, self.hidden_size) - 0.5) * 2)\n",
    "        \n",
    "        self.cosine_dist = nn.CosineSimilarity(dim=2)       \n",
    "        \n",
    "    def forward(self, x, sequence_label, semi, batch_size, hidden_size):\n",
    "        hidden=self.embedding(x.to(device))\n",
    "        M, A = self.self_attention(hidden)\n",
    "        \n",
    "        M_u = M[semi==1]\n",
    "        M_n = M[(semi==0) & (sequence_label==0)]\n",
    "        M_a = M[(semi==0) & (sequence_label==1)]\n",
    "        \n",
    "        M_n = torch.cat((M_u, M_n), dim=0)\n",
    "        \n",
    "        c_n_n = torch.repeat_interleave(self.c_n, M_n.size(0), dim=0)\n",
    "        c_a_n = torch.repeat_interleave(self.c_a, M_n.size(0), dim=0)   \n",
    "        c_a_a = torch.repeat_interleave(self.c_a, M_a.size(0), dim=0)\n",
    "        c_n_a = torch.repeat_interleave(self.c_n, M_a.size(0), dim=0)\n",
    "        \n",
    "        distnn = 0.5 * (1-self.cosine_dist(M_n, c_n_n))\n",
    "        distna = 0.5 * (1-self.cosine_dist(M_n, c_a_n))\n",
    "        distaa = 0.5 * (1-self.cosine_dist(M_a, c_a_a))\n",
    "        distan = 0.5 * (1-self.cosine_dist(M_a, c_n_a))\n",
    "        \n",
    "        context_weights_nn = torch.softmax(-self.alpha*distnn, dim=1)\n",
    "        context_weights_na = torch.softmax(self.alpha*distna, dim=1)\n",
    "        context_weights_aa = torch.softmax(-self.alpha*distaa, dim=1)\n",
    "        context_weights_an = torch.softmax(self.alpha*distan, dim=1)\n",
    "        \n",
    "        dists = (distnn, distna, distaa, distan)\n",
    "        context_weights = (context_weights_nn, context_weights_na, context_weights_aa, context_weights_an)\n",
    "        \n",
    "        triplet_loss1 = torch.sum(distnn*context_weights_nn, dim=1) - torch.sum(distna*context_weights_na, dim=1) + 1\n",
    "        triplet_loss2 = torch.sum(distaa*context_weights_aa, dim=1) - torch.sum(distan*context_weights_an, dim=1) + 1\n",
    "        triplet_loss = torch.sum(torch.relu(triplet_loss1))/(triplet_loss1.size(0)+1) \\\n",
    "                     + torch.sum(torch.relu(triplet_loss2))/(triplet_loss2.size(0)+1)\n",
    "        \n",
    "        return triplet_loss, dists, context_weights, M, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7adb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "ms = CVDDNet(attention_size=attention_size, n_attention_heads=n_attention_heads, hidden_size=hidden_size).to(device)\n",
    "optimizer = optim.Adam(ms.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a60f828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: 149.161444495706\n",
      "Epoch 01: 55.368681963752294\n",
      "Epoch 02: 25.545857317307416\n",
      "Epoch 03: 13.992300931145163\n",
      "Epoch 04: 8.578807774712057\n",
      "Epoch 05: 5.701651124393239\n",
      "Epoch 06: 4.0479360818862915\n",
      "Epoch 07: 3.050789194948533\n",
      "Epoch 08: 2.362859943333794\n",
      "Epoch 09: 1.8974366223110872\n",
      "Epoch 10: 1.5895110824528862\n",
      "Epoch 11: 1.3521536553607267\n",
      "Epoch 12: 1.1611442863941193\n",
      "Epoch 13: 1.0475516845198238\n",
      "Epoch 14: 0.9584555450607749\n",
      "Epoch 15: 0.8474256466416752\n",
      "Epoch 16: 0.791325239574208\n",
      "Epoch 17: 0.7376579747480505\n",
      "Epoch 18: 0.6887844800949097\n",
      "Epoch 19: 0.6498412989518222\n",
      "Epoch 20: 0.623678396729862\n",
      "Epoch 21: 0.5786274618962232\n",
      "Epoch 22: 0.5826650752740747\n",
      "Epoch 23: 0.5466707962400773\n",
      "Epoch 24: 0.5375437526141896\n",
      "Epoch 25: 0.5112924716051888\n",
      "Epoch 26: 0.48286427119198966\n",
      "Epoch 27: 0.4794946996604695\n",
      "Epoch 28: 0.442656290881774\n",
      "Epoch 29: 0.4491191550212748\n",
      "Epoch 30: 0.43766238408930164\n",
      "Epoch 31: 0.42304403641644645\n",
      "Epoch 32: 0.40814367725568657\n",
      "Epoch 33: 0.38683779099408316\n",
      "Epoch 34: 0.3774758270558189\n",
      "Epoch 35: 0.38126858805908875\n",
      "Epoch 36: 0.3601008110186633\n",
      "Epoch 37: 0.3471841487814398\n",
      "Epoch 38: 0.3580931784475551\n",
      "Epoch 39: 0.3587479740381241\n",
      "Epoch 40: 0.3392540084965089\n",
      "Epoch 41: 0.3065727637971149\n",
      "Epoch 42: 0.3022824048119433\n",
      "Epoch 43: 0.2841043897411403\n",
      "Epoch 44: 0.2698334630797891\n",
      "Epoch 45: 0.26286665700814305\n",
      "Epoch 46: 0.25314600853359\n",
      "Epoch 47: 0.23926199139917598\n",
      "Epoch 48: 0.23563799612662373\n",
      "Epoch 49: 0.22716456301072063\n",
      "Epoch 50: 0.21567338792716756\n",
      "Epoch 51: 0.21111797103110483\n",
      "Epoch 52: 0.20239816386910045\n",
      "Epoch 53: 0.2093408887877184\n",
      "Epoch 54: 0.20192864055142684\n",
      "Epoch 55: 0.19032288693329869\n",
      "Epoch 56: 0.18123933059327743\n",
      "Epoch 57: 0.1917282982784159\n",
      "Epoch 58: 0.20789004073423498\n",
      "Epoch 59: 0.21704820280565934\n",
      "Epoch 60: 0.20788245560491786\n",
      "Epoch 61: 0.19657818578621922\n",
      "Epoch 62: 0.19034395060118506\n",
      "Epoch 63: 0.1903364014099626\n",
      "Epoch 64: 0.18416963364271557\n",
      "Epoch 65: 0.18238741071785197\n",
      "Epoch 66: 0.1893556766650256\n",
      "Epoch 67: 0.17857619133942268\n",
      "Epoch 68: 0.1763278064044083\n",
      "Epoch 69: 0.1778840663678506\n",
      "Epoch 70: 0.17266053947455742\n",
      "Epoch 71: 0.16718391438617425\n",
      "Epoch 72: 0.16819927644203692\n",
      "Epoch 73: 0.1677580495529315\n",
      "Epoch 74: 0.16099582130418105\n",
      "Epoch 75: 0.16141004027689204\n",
      "Epoch 76: 0.16358321488780134\n",
      "Epoch 77: 0.16382277055698283\n",
      "Epoch 78: 0.15424362701528213\n",
      "Epoch 79: 0.15539158924537547\n",
      "Epoch 80: 0.1578755196841324\n",
      "Epoch 81: 0.14798015772419817\n",
      "Epoch 82: 0.16773982017355807\n",
      "Epoch 83: 0.17363080075558493\n",
      "Epoch 84: 0.16298983815838308\n",
      "Epoch 85: 0.15831640494220398\n",
      "Epoch 86: 0.1535585566478617\n",
      "Epoch 87: 0.15247498299269116\n",
      "Epoch 88: 0.15643272527000485\n",
      "Epoch 89: 0.2133637725868646\n",
      "Epoch 90: 0.20769327484509525\n",
      "Epoch 91: 0.19402652036617785\n",
      "Epoch 92: 0.18336981556871357\n",
      "Epoch 93: 0.18627550628255396\n",
      "Epoch 94: 0.20391962764894261\n",
      "Epoch 95: 0.19521660489194534\n",
      "Epoch 96: 0.18731583632967053\n",
      "Epoch 97: 0.1938014012925765\n",
      "Epoch 98: 0.19021456316113472\n",
      "Epoch 99: 0.1918665361316765\n",
      "Epoch 100: 0.1941847678493051\n",
      "Epoch 101: 0.18607325260253513\n",
      "Epoch 102: 0.18046368592802217\n",
      "Epoch 103: 0.19527681391028798\n",
      "Epoch 104: 0.1859981279601069\n",
      "Epoch 105: 0.17643499505870483\n",
      "Epoch 106: 0.18264419848428054\n",
      "Epoch 107: 0.18669948113315246\n",
      "Epoch 108: 0.18312854670426426\n",
      "Epoch 109: 0.18449591976754806\n",
      "Epoch 110: 0.18032860164256656\n",
      "Epoch 111: 0.17857220637447693\n",
      "Epoch 112: 0.18263695104157224\n",
      "Epoch 113: 0.18661648977328749\n",
      "Epoch 114: 0.1914871793021174\n",
      "Epoch 115: 0.1900547127075055\n",
      "Epoch 116: 0.1837457685786135\n",
      "Epoch 117: 0.19260765392990672\n",
      "Epoch 118: 0.1737756115548751\n",
      "Epoch 119: 0.17976090903667843\n",
      "Epoch 120: 0.186467614463147\n",
      "Epoch 121: 0.17358306861099074\n",
      "Epoch 122: 0.18409229420563755\n",
      "Epoch 123: 0.1752232593648574\n",
      "Epoch 124: 0.17955206334590912\n",
      "Epoch 125: 0.1839009722804322\n",
      "Epoch 126: 0.17714923883185668\n",
      "Epoch 127: 0.1711903216645998\n",
      "Epoch 128: 0.17515004645375645\n",
      "Epoch 129: 0.18100521056091085\n",
      "Epoch 130: 0.1693418212234974\n",
      "Epoch 131: 0.1767218277296599\n",
      "Epoch 132: 0.1746253533398404\n",
      "Epoch 133: 0.17806993687854095\n",
      "Epoch 134: 0.1661157088682932\n",
      "Epoch 135: 0.17141985476893537\n",
      "Epoch 136: 0.18432949746356292\n",
      "Epoch 137: 0.18142884706749635\n",
      "Epoch 138: 0.18123053386807442\n",
      "Epoch 139: 0.17646761567277067\n",
      "Epoch 140: 0.17321675840546102\n",
      "Epoch 141: 0.24452580380089142\n",
      "Epoch 142: 0.27195389283930554\n",
      "Epoch 143: 0.27717118534971685\n",
      "Epoch 144: 0.2724810297436574\n",
      "Epoch 145: 0.2641054545255268\n",
      "Epoch 146: 0.27604161027599783\n",
      "Epoch 147: 0.265971597722348\n",
      "Epoch 148: 0.2660232103046249\n",
      "Epoch 149: 0.2484579160809517\n"
     ]
    }
   ],
   "source": [
    "setup_seed()\n",
    "\n",
    "best_val_acc_sequence = -1000\n",
    "best_val_acc_entry = -1000\n",
    "best_val_model = None\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    ms.train()\n",
    "    epoch_loss = []  \n",
    "        \n",
    "    for sequence, sequence_label, _, semi in train_loader:\n",
    "        sequence = sequence.to(device)\n",
    "        sequence_label = sequence_label.to(device)\n",
    "        semi = semi.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()      \n",
    "        \n",
    "        triplet_loss, dists, context_weights, M, A = ms(sequence, sequence_label, semi, batch_size_train, hidden_size)        \n",
    "        I = torch.eye(n_attention_heads*2).to(device)\n",
    "        c_na = torch.cat((ms.c_n, ms.c_a), 1)        \n",
    "        CCT = c_na @ c_na.transpose(1, 2)\n",
    "        P = torch.mean((CCT.squeeze() - I) ** 2)\n",
    "        \n",
    "        loss = triplet_loss + lambda_p * P     \n",
    "        loss.backward()       \n",
    "        optimizer.step()       \n",
    "        epoch_loss.append(loss.item())\n",
    "    \n",
    "    ms.eval()\n",
    "    correct_sequence = 0   \n",
    "    correct_entry = 0  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, sequence_label, key_label, semi in val_loader:\n",
    "            pred_key_label_l = []\n",
    "            sequence_label = sequence_label.to(device)\n",
    "            hidden = ms.embedding(sequence.to(device))    \n",
    "            M, A = ms.self_attention(hidden)\n",
    "\n",
    "            n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_val, dim=0)))\n",
    "            a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_val, dim=0)))\n",
    "            n_scores = torch.mean(n_dists, dim=1)\n",
    "            a_scores = torch.mean(a_dists, dim=1) \n",
    "\n",
    "            pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "\n",
    "            _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "            _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "            best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "\n",
    "            best_head_l = best_att_heads.tolist()\n",
    "            index0 = pred_label_batch == 0\n",
    "            index1 = pred_label_batch == 1\n",
    "\n",
    "            for t in range(len(sequence_label)):\n",
    "                pred_key_label_l.append(A[t, best_head_l[t], :].tolist())   \n",
    "\n",
    "            pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "            pred_key_label_t[index0,:] = 0\n",
    "            pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.05, 1.0, 0.0)\n",
    "\n",
    "            acc_sequence = (pred_label_batch==sequence_label).sum().item()\n",
    "            correct_sequence += acc_sequence\n",
    "            acc_entry = (torch.reshape(pred_key_label_t, (-1,))==torch.reshape(key_label,(-1,))).sum().item()\n",
    "            correct_entry += acc_entry\n",
    "                \n",
    "                \n",
    "    if correct_sequence > best_val_acc_sequence and correct_entry > best_val_acc_entry:\n",
    "        best_val_acc_sequence = correct_sequence\n",
    "        best_val_acc_entry = correct_entry\n",
    "        best_val_model = deepcopy(ms.state_dict())\n",
    "        \n",
    "    print(f'Epoch {epoch:02d}: {np.mean(epoch_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93af23",
   "metadata": {},
   "source": [
    "**4. Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e826946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 531.47it/s]\n"
     ]
    }
   ],
   "source": [
    "ms.load_state_dict(best_val_model)\n",
    "ms.eval()\n",
    "\n",
    "pred_seq_label = []\n",
    "true_seq_label = []\n",
    "\n",
    "pred_key_label = []\n",
    "true_key_label = []\n",
    "\n",
    "for sequence, sequence_label, key_label, _ in tqdm(val_loader):\n",
    "    pred_key_label_l = []\n",
    "    true_key_label += torch.reshape(key_label, (-1,)).tolist()\n",
    "    true_seq_label += sequence_label.tolist()\n",
    "    \n",
    "    hidden = ms.embedding(sequence.to(device))    \n",
    "    M, A = ms.self_attention(hidden)\n",
    "    \n",
    "    n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_val, dim=0)))\n",
    "    a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_val, dim=0)))\n",
    "    n_scores = torch.mean(n_dists, dim=1)\n",
    "    a_scores = torch.mean(a_dists, dim=1) \n",
    "    \n",
    "    pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "    pred_seq_label += pred_label_batch.tolist()    \n",
    "    \n",
    "    _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "    _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "    best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "    \n",
    "    best_head_l = best_att_heads.tolist()\n",
    "    index0 = pred_label_batch == 0\n",
    "    index1 = pred_label_batch == 1\n",
    "    \n",
    "    for t in range(len(sequence_label)):\n",
    "        pred_key_label_l.append(A[t, best_head_l[t], :].tolist())\n",
    "        \n",
    "    pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "    pred_key_label_t[index0,:] = 0\n",
    "    pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.05, 1.0, 0.0)\n",
    "    pred_key_label += list(map(int, torch.reshape(pred_key_label_t, (-1,)).tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120067d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       200\n",
      "           1     1.0000    1.0000    1.0000        20\n",
      "\n",
      "    accuracy                         1.0000       220\n",
      "   macro avg     1.0000    1.0000    1.0000       220\n",
      "weighted avg     1.0000    1.0000    1.0000       220\n",
      "\n",
      "[[200   0]\n",
      " [  0  20]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_seq_label, pred_seq_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_seq_label, pred_seq_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccdc160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      4348\n",
      "           1     1.0000    1.0000    1.0000        52\n",
      "\n",
      "    accuracy                         1.0000      4400\n",
      "   macro avg     1.0000    1.0000    1.0000      4400\n",
      "weighted avg     1.0000    1.0000    1.0000      4400\n",
      "\n",
      "[[4348    0]\n",
      " [   0   52]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_key_label, pred_key_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_key_label, pred_key_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_key_label, pred_key_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77f13d",
   "metadata": {},
   "source": [
    "**5. Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc04239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 229.17it/s]\n"
     ]
    }
   ],
   "source": [
    "ms.load_state_dict(best_val_model)\n",
    "ms.eval()\n",
    "\n",
    "pred_seq_label = []\n",
    "true_seq_label = []\n",
    "\n",
    "pred_key_label = []\n",
    "true_key_label = []\n",
    "\n",
    "top_entry = [[] for x in range(n_attention_heads)]\n",
    "\n",
    "for sequence, sequence_label, key_label, _ in tqdm(test_loader):\n",
    "    pred_key_label_l = []\n",
    "    true_key_label += torch.reshape(key_label, (-1,)).tolist()\n",
    "    true_seq_label += sequence_label.tolist()\n",
    "    \n",
    "    hidden = ms.embedding(sequence.to(device))    \n",
    "    M, A = ms.self_attention(hidden)\n",
    "    \n",
    "    n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_test, dim=0)))\n",
    "    a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_test, dim=0)))\n",
    "    n_scores = torch.mean(n_dists, dim=1)\n",
    "    a_scores = torch.mean(a_dists, dim=1) \n",
    "    \n",
    "    pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "    pred_seq_label += pred_label_batch.tolist()    \n",
    "    \n",
    "    _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "    _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "    best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "    \n",
    "    best_head_l = best_att_heads.tolist()\n",
    "    index0 = pred_label_batch == 0\n",
    "    index1 = pred_label_batch == 1\n",
    "    \n",
    "    for t in range(len(sequence_label)):\n",
    "        pred_key_label_l.append(A[t, best_head_l[t], :].tolist())\n",
    "        \n",
    "    pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "    pred_key_label_t[index0,:] = 0\n",
    "    pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.05, 1.0, 0.0)\n",
    "    pred_key_label += list(map(int, torch.reshape(pred_key_label_t, (-1,)).tolist())) \n",
    "\n",
    "    for i in range(batch_size_test):\n",
    "        top_entry[best_head_l[i]] += np.array(sequence[i])[pred_key_label_t.numpy()[i]==1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a15b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9991    0.9995     20000\n",
      "           1     0.9911    0.9985    0.9948      2000\n",
      "\n",
      "    accuracy                         0.9990     22000\n",
      "   macro avg     0.9955    0.9988    0.9971     22000\n",
      "weighted avg     0.9991    0.9990    0.9990     22000\n",
      "\n",
      "[[19982    18]\n",
      " [    3  1997]]\n",
      "0.9987999999999999\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_seq_label, pred_seq_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_seq_label, pred_seq_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Sequence anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(true_seq_label, pred_seq_label))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2555684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000    434874\n",
      "           1     0.9992    0.9988    0.9990      5126\n",
      "\n",
      "    accuracy                         1.0000    440000\n",
      "   macro avg     0.9996    0.9994    0.9995    440000\n",
      "weighted avg     1.0000    1.0000    1.0000    440000\n",
      "\n",
      "[[434870      4]\n",
      " [     6   5120]]\n",
      "0.9994101493085038\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_key_label, pred_key_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_key_label, pred_key_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_key_label, pred_key_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(true_key_label, pred_key_label, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(true_key_label, pred_key_label))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.write('-'*50 + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1251d9",
   "metadata": {},
   "source": [
    "**6. Top entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48e4306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('top_entry.txt', 'w+')\n",
    "\n",
    "for i in range(n_attention_heads):\n",
    "    f.write('Head ' + str(i) + ': ' + '\\n' )\n",
    "    f.write(str(Counter(top_entry[i]).most_common()))\n",
    "    f.write('\\n'*2)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de7f5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a8ec9a56', 5120), ('d4273323', 4), ('296d2788', 1), ('71db4dcd', 1)]\n"
     ]
    }
   ],
   "source": [
    "abnormal_keys = []\n",
    "\n",
    "for i in range(test_abnormal.shape[0]):\n",
    "    abnormal_keys += test_abnormal.iloc[i, 0][test_abnormal.iloc[i, 2]==1].tolist()\n",
    "    \n",
    "print(Counter(abnormal_keys).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc6c76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[87, 5120], [None, 4], [None, 1], [None, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal_key2index = []\n",
    "\n",
    "for each in Counter(abnormal_keys).most_common():\n",
    "    tmp = list(each)\n",
    "    tmp[0] = logkey2index.get(each[0])\n",
    "    abnormal_key2index.append(tmp)\n",
    "    \n",
    "abnormal_key2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8ea38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
