{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27ca417",
   "metadata": {},
   "source": [
    "**1. Read csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61abe436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cea5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cd7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "def setup_seed(seed=seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008c6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('output.txt', 'a')\n",
    "f.write('seed: ' + str(seed) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata_all = pd.read_csv(r'~/Python_projects/Rationale/Dataset/BGL.log_structured_v1.csv')\n",
    "logdata = logdata_all[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197a6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7910aa0897e6>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: 0 if x == '-' else 1)\n"
     ]
    }
   ],
   "source": [
    "def slide_window(logdata, window_size = 20, step_size = 10):\n",
    "#     logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: int(x != '-'))\n",
    "    logdata[\"Label\"] = logdata[\"Label\"].apply(lambda x: 0 if x == '-' else 1)  \n",
    "    data = logdata.loc[:, ['EventId', 'Label']]\n",
    "    data['Key_label'] = data['Label']\n",
    "    data.rename(columns={'Label':'Sequence_label'})\n",
    "    logkey = data['EventId']\n",
    "    logkey_label = data['Key_label']\n",
    "\n",
    "    new_data = []\n",
    "    idx = 0\n",
    "\n",
    "    while idx <= data.shape[0] - window_size:\n",
    "        new_data.append([logkey[idx : idx+window_size].values,\n",
    "                         max(logkey_label[idx : idx+window_size]),\n",
    "                         logkey_label[idx : idx+window_size].values\n",
    "                        ])\n",
    "        idx += step_size\n",
    "    return pd.DataFrame(new_data, columns = ['EventId', 'Sequence_label', 'Key_label'])\n",
    "\n",
    "dataset = slide_window(logdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec473de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75336 24663\n"
     ]
    }
   ],
   "source": [
    "setup_seed()\n",
    "\n",
    "n_labeled = 20\n",
    "n_unlabeled = 2000\n",
    "a_unlabeled = 0\n",
    "\n",
    "normal_data = dataset[dataset['Sequence_label']==0]\n",
    "abnormal_data = dataset[dataset['Sequence_label']==1]\n",
    "print(normal_data.shape[0], abnormal_data.shape[0])\n",
    "\n",
    "# train data\n",
    "train_normal_all = normal_data.sample(n= n_unlabeled + n_labeled, random_state=seed)\n",
    "train_abnormal_all = abnormal_data.sample(n= a_unlabeled + n_labeled, random_state=seed)\n",
    "\n",
    "train_normal_labeled = train_normal_all.sample(n=n_labeled, random_state=seed)\n",
    "train_abnormal_labeled = train_abnormal_all.sample(n=n_labeled, random_state=seed)\n",
    "\n",
    "train_normal_unlabeled = train_normal_all.drop(train_normal_labeled.index)\n",
    "train_abnormal_unlabeled = train_abnormal_all.drop(train_abnormal_labeled.index)\n",
    "train_unlabeled = pd.concat([train_normal_unlabeled, train_abnormal_unlabeled])\n",
    "\n",
    "train_normal_labeled['Semi'] = 0\n",
    "train_abnormal_labeled['Semi'] = 0\n",
    "train_unlabeled['Semi'] = 1\n",
    "train_ds = pd.concat([train_normal_labeled, train_abnormal_labeled, train_unlabeled])\n",
    "\n",
    "rest_normal = normal_data.drop(train_normal_all.index)\n",
    "rest_abnormal = abnormal_data.drop(train_abnormal_all.index)\n",
    "\n",
    "# validation data\n",
    "val_normal = rest_normal.sample(n=200, random_state=seed)\n",
    "val_abnormal = rest_abnormal.sample(n=20, random_state=seed)\n",
    "\n",
    "val_ds = pd.concat([val_normal, val_abnormal])\n",
    "val_ds['Semi'] = 0\n",
    "\n",
    "# testing data\n",
    "test_normal = rest_normal.drop(val_normal.index).sample(n=20000, random_state=seed)\n",
    "test_abnormal = rest_abnormal.drop(val_abnormal.index).sample(n=2000, random_state=seed)\n",
    "\n",
    "test_ds = pd.concat([test_normal, test_abnormal])\n",
    "test_ds['Semi'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828dc59",
   "metadata": {},
   "source": [
    "**2. Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ac905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "\n",
    "for index, row in train_ds.iterrows():\n",
    "    counts.update(row['EventId'])\n",
    "    \n",
    "logkey2index ={\"\":0,\"UNK\":1}\n",
    "logkeys = [\"\",\"UNK\"]\n",
    "\n",
    "for word in counts:\n",
    "    logkey2index[word] = len(logkeys)\n",
    "    logkeys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa010795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, logkey2index):\n",
    "    return np.array([logkey2index.get(logkey, logkey2index[\"UNK\"]) for logkey in sequence])\n",
    "\n",
    "train_ds.loc[:,'Encoded'] = train_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))\n",
    "val_ds.loc[:,'Encoded'] = val_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))\n",
    "test_ds.loc[:,'Encoded'] = test_ds.loc[:,'EventId'].apply(lambda x: encode_sequence(x, logkey2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3cc005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 60\n",
    "batch_size_val = 20\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38a2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "train_data_list = []\n",
    "for i in range(train_ds.shape[0]):\n",
    "    train_data_list.append([train_ds.iloc[i, 4].tolist(), train_ds.iloc[i, 1], \\\n",
    "                            train_ds.iloc[i, 2].tolist(), train_ds.iloc[i, 3]])\n",
    "    \n",
    "val_data_list = []\n",
    "for i in range(val_ds.shape[0]):\n",
    "    val_data_list.append([val_ds.iloc[i, 4].tolist(), val_ds.iloc[i, 1], \\\n",
    "                            val_ds.iloc[i, 2].tolist(), val_ds.iloc[i, 3]])\n",
    "    \n",
    "test_data_list = []\n",
    "for i in range(test_ds.shape[0]):\n",
    "    test_data_list.append([test_ds.iloc[i, 4].tolist(), test_ds.iloc[i, 1], \\\n",
    "                            test_ds.iloc[i, 2].tolist(), test_ds.iloc[i, 3]])\n",
    "\n",
    "\n",
    "def collate_fn(data_list):\n",
    "    sequence = torch.tensor([x[0] for x in data_list])\n",
    "    sequence_label = torch.tensor([x[1] for x in data_list])\n",
    "    key_label = torch.tensor([x[2] for x in data_list])\n",
    "    semi = torch.tensor([x[3] for x in data_list])\n",
    "    return sequence, sequence_label, key_label, semi\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size = batch_size_train, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_data_list, batch_size = batch_size_val, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size = batch_size_test, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4c905",
   "metadata": {},
   "source": [
    "**3. Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20998cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "lambda_p = 1\n",
    "hidden_size = 150\n",
    "attention_size = 300\n",
    "n_attention_heads = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba959334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=hidden_size, attention_size=attention_size, n_attention_heads=n_attention_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_size = attention_size\n",
    "        self.n_attention_heads = n_attention_heads\n",
    "        self.W1 = nn.Linear(hidden_size, attention_size, bias=True)\n",
    "        self.W2 = nn.Linear(attention_size, n_attention_heads, bias=True)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        x = torch.tanh(self.W1(hidden))\n",
    "        # x.shape = (batch_size, sentence_length, attention_size)\n",
    "        \n",
    "        x = F.softmax(self.W2(x), dim=1)  # softmax over sentence_length\n",
    "        # x.shape = (batch_size, sentence_length, n_attention_heads)\n",
    "\n",
    "        A = x.transpose(1, 2)\n",
    "        M = A @ hidden\n",
    "        # A.shape = (batch_size, n_attention_heads, sentence_length)\n",
    "        # M.shape = (batch_size, n_attention_heads, hidden_size)\n",
    "\n",
    "        return M, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9d4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVDDNet(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_size, n_attention_heads, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_size = attention_size\n",
    "        self.n_attention_heads = n_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = 1.0\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(logkeys), embedding_dim=hidden_size)  \n",
    "        self.self_attention = SelfAttention(hidden_size=self.hidden_size,\n",
    "                                            attention_size=attention_size,\n",
    "                                            n_attention_heads=n_attention_heads) \n",
    "        \n",
    "#         self.c_n = nn.Parameter(torch.repeat_interleave((torch.rand(1, 1, self.hidden_size) - 0.5) * 2, n_attention_heads, dim=1))\n",
    "        self.c_n = nn.Parameter((torch.rand(1, n_attention_heads, self.hidden_size) - 0.5) * 2)\n",
    "        self.c_a = nn.Parameter((torch.rand(1, n_attention_heads, self.hidden_size) - 0.5) * 2)\n",
    "        \n",
    "        self.cosine_dist = nn.CosineSimilarity(dim=2)       \n",
    "        \n",
    "    def forward(self, x, sequence_label, semi, batch_size, hidden_size):\n",
    "        hidden=self.embedding(x.to(device))\n",
    "        M, A = self.self_attention(hidden)\n",
    "        \n",
    "        M_u = M[semi==1]\n",
    "        M_n = M[(semi==0) & (sequence_label==0)]\n",
    "        M_a = M[(semi==0) & (sequence_label==1)]\n",
    "#         print(M_u.size(), M_n.size(), M_a.size())\n",
    "        M_n = torch.cat((M_u, M_n), dim=0)\n",
    "        \n",
    "        c_n_n = torch.repeat_interleave(self.c_n, M_n.size(0), dim=0)\n",
    "        c_a_n = torch.repeat_interleave(self.c_a, M_n.size(0), dim=0)   \n",
    "        c_a_a = torch.repeat_interleave(self.c_a, M_a.size(0), dim=0)\n",
    "        c_n_a = torch.repeat_interleave(self.c_n, M_a.size(0), dim=0)\n",
    "        \n",
    "        distnn = 0.5 * (1-self.cosine_dist(M_n, c_n_n))\n",
    "        distna = 0.5 * (1-self.cosine_dist(M_n, c_a_n))\n",
    "        distaa = 0.5 * (1-self.cosine_dist(M_a, c_a_a))\n",
    "        distan = 0.5 * (1-self.cosine_dist(M_a, c_n_a))\n",
    "        \n",
    "        context_weights_nn = torch.softmax(-self.alpha*distnn, dim=1)\n",
    "        context_weights_na = torch.softmax(self.alpha*distna, dim=1)\n",
    "        context_weights_aa = torch.softmax(-self.alpha*distaa, dim=1)\n",
    "        context_weights_an = torch.softmax(self.alpha*distan, dim=1)\n",
    "        \n",
    "        dists = (distnn, distna, distaa, distan)\n",
    "        context_weights = (context_weights_nn, context_weights_na, context_weights_aa, context_weights_an)\n",
    "        \n",
    "        triplet_loss1 = torch.sum(distnn*context_weights_nn, dim=1) - torch.sum(distna*context_weights_na, dim=1) + 1\n",
    "        triplet_loss2 = torch.sum(distaa*context_weights_aa, dim=1) - torch.sum(distan*context_weights_an, dim=1) + 1\n",
    "        triplet_loss = torch.sum(torch.relu(triplet_loss1))/(triplet_loss1.size(0)+1) \\\n",
    "                     + torch.sum(torch.relu(triplet_loss2))/(triplet_loss2.size(0)+1)\n",
    "        \n",
    "        return triplet_loss, dists, context_weights, M, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7adb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed()\n",
    "\n",
    "ms = CVDDNet(attention_size=attention_size, n_attention_heads=n_attention_heads, hidden_size=hidden_size).to(device)\n",
    "optimizer = optim.Adam(ms.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a60f828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: 186.6441805222455\n",
      "Epoch 01: 71.01878693524529\n",
      "Epoch 02: 33.20840297025793\n",
      "Epoch 03: 18.307571719674502\n",
      "Epoch 04: 11.262565528645235\n",
      "Epoch 05: 7.5013454100664925\n",
      "Epoch 06: 5.325543095083797\n",
      "Epoch 07: 4.011287065113292\n",
      "Epoch 08: 3.113211386344012\n",
      "Epoch 09: 2.5057727168588078\n",
      "Epoch 10: 2.1023060398943283\n",
      "Epoch 11: 1.7979145050048828\n",
      "Epoch 12: 1.550485726665048\n",
      "Epoch 13: 1.4030431509017944\n",
      "Epoch 14: 1.282876175992629\n",
      "Epoch 15: 1.146555294008816\n",
      "Epoch 16: 1.0736668583224802\n",
      "Epoch 17: 0.9958354760618771\n",
      "Epoch 18: 0.9316378744209514\n",
      "Epoch 19: 0.8874908668153426\n",
      "Epoch 20: 0.8618726221954122\n",
      "Epoch 21: 0.8147013485431671\n",
      "Epoch 22: 0.8063702688497656\n",
      "Epoch 23: 0.7671512733487522\n",
      "Epoch 24: 0.7482166377937093\n",
      "Epoch 25: 0.7151724065051359\n",
      "Epoch 26: 0.6848171476055595\n",
      "Epoch 27: 0.6746900660150191\n",
      "Epoch 28: 0.6347763626014485\n",
      "Epoch 29: 0.6365587220472448\n",
      "Epoch 30: 0.6247434633619645\n",
      "Epoch 31: 0.6101397363578572\n",
      "Epoch 32: 0.5875925255172393\n",
      "Epoch 33: 0.5589508624637828\n",
      "Epoch 34: 0.5389444591367946\n",
      "Epoch 35: 0.5354688807445414\n",
      "Epoch 36: 0.5004249539445428\n",
      "Epoch 37: 0.4768628004719229\n",
      "Epoch 38: 0.4531647107180427\n",
      "Epoch 39: 0.42296527939684253\n",
      "Epoch 40: 0.3896949037032969\n",
      "Epoch 41: 0.35206735221778646\n",
      "Epoch 42: 0.32626023537972393\n",
      "Epoch 43: 0.2848536556257921\n",
      "Epoch 44: 0.25152383525581923\n",
      "Epoch 45: 0.22129176732371836\n",
      "Epoch 46: 0.19171031082377715\n",
      "Epoch 47: 0.16774770486004212\n",
      "Epoch 48: 0.15234941244125366\n",
      "Epoch 49: 0.13648055384264274\n",
      "Epoch 50: 0.12565027221160777\n",
      "Epoch 51: 0.11852289911578684\n",
      "Epoch 52: 0.10865710325100843\n",
      "Epoch 53: 0.10504308562068378\n",
      "Epoch 54: 0.10417302478762235\n",
      "Epoch 55: 0.10028254591366824\n",
      "Epoch 56: 0.1001340909039273\n",
      "Epoch 57: 0.0999681327272864\n",
      "Epoch 58: 0.09734418725266177\n",
      "Epoch 59: 0.09873685525620685\n",
      "Epoch 60: 0.09755789269419278\n",
      "Epoch 61: 0.09690534082405708\n",
      "Epoch 62: 0.09680227694265983\n",
      "Epoch 63: 0.097894378225593\n",
      "Epoch 64: 0.09613634382977206\n",
      "Epoch 65: 0.09481144170550738\n",
      "Epoch 66: 0.09770241752266884\n",
      "Epoch 67: 0.09678060013581724\n",
      "Epoch 68: 0.09680318306474124\n",
      "Epoch 69: 0.09590734037406304\n",
      "Epoch 70: 0.09758611943791895\n",
      "Epoch 71: 0.09608274961219114\n",
      "Epoch 72: 0.09754576687427129\n",
      "Epoch 73: 0.09670448215568767\n",
      "Epoch 74: 0.09667206336470212\n",
      "Epoch 75: 0.09623903408646584\n",
      "Epoch 76: 0.09626800106728778\n",
      "Epoch 77: 0.09749292056350146\n",
      "Epoch 78: 0.09582293011686381\n",
      "Epoch 79: 0.09665882653173279\n",
      "Epoch 80: 0.09747294336557388\n",
      "Epoch 81: 0.0939023325986722\n",
      "Epoch 82: 0.09663204761112437\n",
      "Epoch 83: 0.09497873265953626\n",
      "Epoch 84: 0.09621981443727717\n",
      "Epoch 85: 0.0966438521795413\n",
      "Epoch 86: 0.09746234338073169\n",
      "Epoch 87: 0.09580449212123365\n",
      "Epoch 88: 0.09539561280432869\n",
      "Epoch 89: 0.09745017241905718\n",
      "Epoch 90: 0.09579067984048058\n",
      "Epoch 91: 0.09661759151255384\n",
      "Epoch 92: 0.0936418131870382\n",
      "Epoch 93: 0.0957902807961492\n",
      "Epoch 94: 0.09743950603639379\n",
      "Epoch 95: 0.09619900847182554\n",
      "Epoch 96: 0.09661208005512462\n",
      "Epoch 97: 0.09661040937199313\n",
      "Epoch 98: 0.09661209692849833\n",
      "Epoch 99: 0.09662006072261754\n",
      "Epoch 100: 0.09661268990705996\n",
      "Epoch 101: 0.0953715649597785\n",
      "Epoch 102: 0.09578790953930687\n",
      "Epoch 103: 0.09578640386462212\n",
      "Epoch 104: 0.0966138000435689\n",
      "Epoch 105: 0.09619873937438517\n",
      "Epoch 106: 0.09578303072382421\n",
      "Epoch 107: 0.0966080083128284\n",
      "Epoch 108: 0.09578190919230967\n",
      "Epoch 109: 0.09660710723084562\n",
      "Epoch 110: 0.09660753520096049\n",
      "Epoch 111: 0.09454295039176941\n",
      "Epoch 112: 0.0966092639547937\n",
      "Epoch 113: 0.09660827149363126\n",
      "Epoch 114: 0.09537303447723389\n",
      "Epoch 115: 0.09662874525084215\n",
      "Epoch 116: 0.09537579425994087\n",
      "Epoch 117: 0.09661101155421313\n",
      "Epoch 118: 0.09537489449276644\n",
      "Epoch 119: 0.09743347479140058\n",
      "Epoch 120: 0.09743381927118581\n",
      "Epoch 121: 0.09536929275183116\n",
      "Epoch 122: 0.09660712651470128\n",
      "Epoch 123: 0.09619399927118245\n",
      "Epoch 124: 0.09578134426299263\n",
      "Epoch 125: 0.09660934459637194\n",
      "Epoch 126: 0.09594885018818519\n",
      "Epoch 127: 0.09661771685761564\n",
      "Epoch 128: 0.09578705600955907\n",
      "Epoch 129: 0.09743641579852384\n",
      "Epoch 130: 0.09661573697538937\n",
      "Epoch 131: 0.09661933429100934\n",
      "Epoch 132: 0.09457279018619481\n",
      "Epoch 133: 0.09743888566599172\n",
      "Epoch 134: 0.09429914789164767\n",
      "Epoch 135: 0.09512750870164703\n",
      "Epoch 136: 0.09743575444992851\n",
      "Epoch 137: 0.0974338977214168\n",
      "Epoch 138: 0.09743422773831031\n",
      "Epoch 139: 0.09661084632663165\n",
      "Epoch 140: 0.09660781262552037\n",
      "Epoch 141: 0.09660653528921745\n",
      "Epoch 142: 0.09495497210060849\n",
      "Epoch 143: 0.09454188780749545\n",
      "Epoch 144: 0.09495474573443918\n",
      "Epoch 145: 0.09660619891741697\n",
      "Epoch 146: 0.09743750489809934\n",
      "Epoch 147: 0.09743581756072886\n",
      "Epoch 148: 0.09619416844318895\n",
      "Epoch 149: 0.09660649409188944\n"
     ]
    }
   ],
   "source": [
    "setup_seed()\n",
    "\n",
    "best_val_acc_sequence = -1000\n",
    "best_val_acc_entry = -1000\n",
    "best_val_model = None\n",
    "\n",
    "for epoch in range(epochs):  \n",
    "    ms.train()\n",
    "    epoch_loss = []  \n",
    "        \n",
    "    for sequence, sequence_label, _, semi in train_loader:\n",
    "        sequence = sequence.to(device)\n",
    "        sequence_label = sequence_label.to(device)\n",
    "        semi = semi.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()      \n",
    "        \n",
    "        triplet_loss, dists, context_weights, M, A = ms(sequence, sequence_label, semi, batch_size_train, hidden_size)        \n",
    "        I = torch.eye(n_attention_heads*2).to(device)\n",
    "        c_na = torch.cat((ms.c_n, ms.c_a), 1)        \n",
    "        CCT = c_na @ c_na.transpose(1, 2)\n",
    "        P = torch.mean((CCT.squeeze() - I) ** 2)\n",
    "        \n",
    "        loss = triplet_loss + lambda_p * P     \n",
    "        loss.backward()       \n",
    "        optimizer.step()       \n",
    "        epoch_loss.append(loss.item())\n",
    "    \n",
    "    ms.eval()\n",
    "    correct_sequence = 0   \n",
    "    correct_entry = 0  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, sequence_label, key_label, semi in val_loader:\n",
    "            pred_key_label_l = []\n",
    "            sequence_label = sequence_label.to(device)\n",
    "            hidden = ms.embedding(sequence.to(device))    \n",
    "            M, A = ms.self_attention(hidden)\n",
    "\n",
    "            n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_val, dim=0)))\n",
    "            a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_val, dim=0)))\n",
    "            n_scores = torch.mean(n_dists, dim=1)\n",
    "            a_scores = torch.mean(a_dists, dim=1) \n",
    "\n",
    "            pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "\n",
    "            _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "            _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "            best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "\n",
    "            best_head_l = best_att_heads.tolist()\n",
    "            index0 = pred_label_batch == 0\n",
    "            index1 = pred_label_batch == 1\n",
    "\n",
    "            for t in range(len(sequence_label)):\n",
    "                pred_key_label_l.append(A[t, best_head_l[t], :].tolist())   \n",
    "\n",
    "            pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "            pred_key_label_t[index0,:] = 0\n",
    "            pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.01, 1.0, 0.0)\n",
    "\n",
    "            acc_sequence = (pred_label_batch==sequence_label).sum().item()\n",
    "            correct_sequence += acc_sequence\n",
    "            acc_entry = (torch.reshape(pred_key_label_t, (-1,))==torch.reshape(key_label,(-1,))).sum().item()\n",
    "            correct_entry += acc_entry\n",
    "                \n",
    "                \n",
    "    if correct_sequence > best_val_acc_sequence and correct_entry > best_val_acc_entry:\n",
    "        best_val_acc_sequence = correct_sequence\n",
    "        best_val_acc_entry = correct_entry\n",
    "        best_val_model = deepcopy(ms.state_dict())\n",
    "        \n",
    "    print(f'Epoch {epoch:02d}: {np.mean(epoch_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246ff5c",
   "metadata": {},
   "source": [
    "**4. Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f4e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 205.13it/s]\n"
     ]
    }
   ],
   "source": [
    "ms.load_state_dict(best_val_model)\n",
    "ms.eval()\n",
    "\n",
    "pred_seq_label = []\n",
    "true_seq_label = []\n",
    "\n",
    "pred_key_label = []\n",
    "true_key_label = []\n",
    "\n",
    "for sequence, sequence_label, key_label, _ in tqdm(val_loader):\n",
    "    pred_key_label_l = []\n",
    "    true_key_label += torch.reshape(key_label, (-1,)).tolist()\n",
    "    true_seq_label += sequence_label.tolist()\n",
    "    \n",
    "    hidden = ms.embedding(sequence.to(device))    \n",
    "    M, A = ms.self_attention(hidden)\n",
    "    \n",
    "    n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_val, dim=0)))\n",
    "    a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_val, dim=0)))\n",
    "    n_scores = torch.mean(n_dists, dim=1)\n",
    "    a_scores = torch.mean(a_dists, dim=1) \n",
    "    \n",
    "    pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "    pred_seq_label += pred_label_batch.tolist()    \n",
    "    \n",
    "    _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "    _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "    best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "    \n",
    "    best_head_l = best_att_heads.tolist()\n",
    "    index0 = pred_label_batch == 0\n",
    "    index1 = pred_label_batch == 1\n",
    "    \n",
    "    for t in range(len(sequence_label)):\n",
    "        pred_key_label_l.append(A[t, best_head_l[t], :].tolist())\n",
    "        \n",
    "    pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "    pred_key_label_t[index0,:] = 0\n",
    "    pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.01, 1.0, 0.0)\n",
    "    pred_key_label += list(map(int, torch.reshape(pred_key_label_t, (-1,)).tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fccea2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    1.0000    0.9975       200\n",
      "           1     1.0000    0.9500    0.9744        20\n",
      "\n",
      "    accuracy                         0.9955       220\n",
      "   macro avg     0.9975    0.9750    0.9859       220\n",
      "weighted avg     0.9955    0.9955    0.9954       220\n",
      "\n",
      "[[200   0]\n",
      " [  1  19]]\n",
      "0.975\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_seq_label, pred_seq_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_seq_label, pred_seq_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66cb9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9978    1.0000    0.9989      4030\n",
      "           1     1.0000    0.9757    0.9877       370\n",
      "\n",
      "    accuracy                         0.9980      4400\n",
      "   macro avg     0.9989    0.9878    0.9933      4400\n",
      "weighted avg     0.9980    0.9980    0.9979      4400\n",
      "\n",
      "[[4030    0]\n",
      " [   9  361]]\n",
      "0.9878378378378379\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_key_label, pred_key_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_key_label, pred_key_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_key_label, pred_key_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aac643",
   "metadata": {},
   "source": [
    "**5. Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc04239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 23.23it/s]\n"
     ]
    }
   ],
   "source": [
    "ms.load_state_dict(best_val_model)\n",
    "ms.eval()\n",
    "\n",
    "pred_seq_label = []\n",
    "true_seq_label = []\n",
    "\n",
    "pred_key_label = []\n",
    "true_key_label = []\n",
    "\n",
    "top_entry = [[] for x in range(n_attention_heads)]\n",
    "\n",
    "for sequence, sequence_label, key_label, _ in tqdm(test_loader):\n",
    "    pred_key_label_l = []\n",
    "    true_key_label += torch.reshape(key_label, (-1,)).tolist()\n",
    "    true_seq_label += sequence_label.tolist()\n",
    "    \n",
    "    hidden = ms.embedding(sequence.to(device))    \n",
    "    M, A = ms.self_attention(hidden)\n",
    "    \n",
    "    n_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_n, batch_size_test, dim=0)))\n",
    "    a_dists = 0.5 * (1-ms.cosine_dist(M, torch.repeat_interleave(ms.c_a, batch_size_test, dim=0)))\n",
    "    n_scores = torch.mean(n_dists, dim=1)\n",
    "    a_scores = torch.mean(a_dists, dim=1) \n",
    "    \n",
    "    pred_label_batch = torch.where(n_scores<a_scores, 0, 1)\n",
    "    pred_seq_label += pred_label_batch.tolist()    \n",
    "    \n",
    "    _, n_best_heads = torch.min(n_dists, dim=1)\n",
    "    _, a_best_heads = torch.min(a_dists, dim=1)\n",
    "    best_att_heads = torch.where(pred_label_batch==0, n_best_heads, a_best_heads)\n",
    "    \n",
    "    best_head_l = best_att_heads.tolist()\n",
    "    index0 = pred_label_batch == 0\n",
    "    index1 = pred_label_batch == 1\n",
    "    \n",
    "    for t in range(len(sequence_label)):\n",
    "        pred_key_label_l.append(A[t, best_head_l[t], :].tolist())\n",
    "        \n",
    "    pred_key_label_t = torch.tensor(pred_key_label_l)\n",
    "    pred_key_label_t[index0,:] = 0\n",
    "    pred_key_label_t[index1] = torch.where(pred_key_label_t[index1]>0.01, 1.0, 0.0)\n",
    "    pred_key_label += list(map(int, torch.reshape(pred_key_label_t, (-1,)).tolist())) \n",
    "\n",
    "    for i in range(batch_size_test):\n",
    "        top_entry[best_head_l[i]] += np.array(sequence[i])[pred_key_label_t.numpy()[i]==1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a15b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9931    0.9981    0.9956     20000\n",
      "           1     0.9805    0.9305    0.9548      2000\n",
      "\n",
      "    accuracy                         0.9920     22000\n",
      "   macro avg     0.9868    0.9643    0.9752     22000\n",
      "weighted avg     0.9919    0.9920    0.9919     22000\n",
      "\n",
      "[[19963    37]\n",
      " [  139  1861]]\n",
      "0.9643249999999999\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_seq_label, pred_seq_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_seq_label, pred_seq_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Sequence anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(true_seq_label, pred_seq_label, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(true_seq_label, pred_seq_label))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2555684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9986    0.9978    0.9982    403887\n",
      "           1     0.9761    0.9839    0.9800     36113\n",
      "\n",
      "    accuracy                         0.9967    440000\n",
      "   macro avg     0.9873    0.9909    0.9891    440000\n",
      "weighted avg     0.9967    0.9967    0.9967    440000\n",
      "\n",
      "[[403018    869]\n",
      " [   583  35530]]\n",
      "0.9908523185765655\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(true_key_label, pred_key_label, digits=4))\n",
    "print(metrics.confusion_matrix(true_key_label, pred_key_label))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_key_label, pred_key_label, pos_label=1)\n",
    "print(metrics.auc(fpr, tpr))\n",
    "\n",
    "f = open('output.txt', 'a')\n",
    "f.write('Entry anomaly detection on detected sequences:'+'\\n')\n",
    "f.write(str(metrics.classification_report(true_key_label, pred_key_label, digits=4))+'\\n')\n",
    "f.write(str(metrics.confusion_matrix(true_key_label, pred_key_label))+'\\n')\n",
    "f.write(str(metrics.auc(fpr, tpr))+'\\n')\n",
    "f.write('-'*50 + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee657c",
   "metadata": {},
   "source": [
    "**6. Top entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a3d88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('top_entry.txt', 'w+')\n",
    "\n",
    "for i in range(n_attention_heads):\n",
    "    f.write('Head ' + str(i) + ': ' + '\\n' )\n",
    "    f.write(str(Counter(top_entry[i]).most_common()))\n",
    "    f.write('\\n'*2)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff107dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('38a7307d', 24245), ('d2c9db9b', 10249), ('150b1306', 1040), ('220716fc', 468), ('4496b375', 108), ('ce2b6cdc', 2), ('79913dac', 1)]\n"
     ]
    }
   ],
   "source": [
    "abnormal_keys = []\n",
    "\n",
    "for i in range(test_abnormal.shape[0]):\n",
    "    abnormal_keys += test_abnormal.iloc[i, 0][test_abnormal.iloc[i, 2]==1].tolist()\n",
    "    \n",
    "print(Counter(abnormal_keys).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91af4cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 24245],\n",
       " [19, 10249],\n",
       " [None, 1040],\n",
       " [None, 468],\n",
       " [None, 108],\n",
       " [40, 2],\n",
       " [None, 1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal_key2index = []\n",
    "\n",
    "for each in Counter(abnormal_keys).most_common():\n",
    "    tmp = list(each)\n",
    "    tmp[0] = logkey2index.get(each[0])\n",
    "    abnormal_key2index.append(tmp)\n",
    "    \n",
    "abnormal_key2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eeb816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
